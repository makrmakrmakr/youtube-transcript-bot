- English (auto-generated)

- Generated by YouTube: True

foreign
good morning good afternoon and good
evening to everyone and thank you for
joining our second monthly life number
15.
this is our first meet up for the year
and I'm really glad that so many people
joined
so before I tell you I'm gonna switch my
phone first
before I tell you more about our
yesterday which you've met I guess
everybody met this is Joe not evident
whose CEO and co-founder of that here
um and before I tell you more about his
presentation as always
um I am about to share some interesting
events which are coming up as you know
we have an initiative which is called
events and the idea is to announce
interesting events around data open data
and open source
I've prepared three events although
there are plenty more
um but I'll just mention them and then
if you're interested
um you can find maybe I should drop the
link again so the link to the meeting
just one second
um I'll just drop the link for the new
cameras because I and I know that there
is no history so you can find all things
in the meeting notes document and in the
presentation that you see on screen
so the first event is
um data Team Summit which is an annual
virtual community event basically it
gathers data professionals and experts
um like data Engineers administrators
Architects analysts
and they discuss that Ops related stuff
it's a free event and I think you can
still uh if you're interested in
speaking you can still join the next
event is a conference which is around
open data and energy analytics analytics
it's in February and probably I'll
mention it next time as well it is in
Sydney Australia and um digital as well
so basically if you're a researcher or
you do some some research around open
data and energy analytics you have the
opportunity to participate and to submit
your paper as well
and the next event and last one is
pretty like it's in March but I want to
start speaking about that
um in now because
um it's there is a call for proposal so
basically you can
it's a big event it's open date a week
we all know it it happens usually in
March
um you can you can submit your proposals
if you want to participate
um it's an annual Festival
um it's Community Driven and it's
organized by betta NYC that the true
design and other people on the agenda
for today is we're going to have a
presentation from Joel natividad who's
gonna speak about data Pusher plus this
is a fork of data Pusher and Joe will
tell us more about second data store
background
current waste data is loaded into the
second data store and how data Pusher
plus improves on these existing methods
and what additional capabilities it
unlocks
after Joe's presentation we're going to
have a q a session
um and we're going to open the floor for
questions
there are two ways to ask a question you
can raise a hand and go ahead or you can
drop questions in the chat
after the Q a session as always we're
going to have Community announcements
and sharing for those of you who are not
aware if you want to share something
with the community if you have some
interesting project or anything or you
want to participate maybe in future life
you can this is the time to to to share
with us all right so if you want to add
things to the agenda
um there is the link to the meeting note
in the chat I'll share it one more time
because there are newcomers joining
after I started
so you can
um there is a meeting though where you
can suggest you can either add things to
the agenda or you can suggest a topic
um you can also leave your contact
information I encourage you to do that
because we are here to connect as well
so that can be useful for other people
and there is a
special space where you can leave your
contact details okay and
yeah just reflect the people that the
meetings be recorded it will be uploaded
to our YouTube channel I think I can
give the mic to Joel now
um you're a co-host and you should be
able to share your screen let me know
can you hear me
yeah and then I was still presenting
this
okay thanks Joanna
so yes welcome to 2023 so
uh last year has been a great year for
CCAD and I look forward to more
developments and Innovations at the
secant community
so one contribution we have is uh data
Pusher plus so let me just uh present it
now quickly
okay so data Pusher plus uh taking the
secant that is stored to the next level
I don't think I need to explain what the
data story is everybody who's here must
have played around with it but let me
give a little history about how we got
here
so I'm sure as Joanna said data Pusher
plus is a Ford Code data pusher data
Pusher is one of the
this core components of secan that is
used to populate the data store you know
it uses this component that called
message tables unfortunately name I
think uh and it that's inferencing of
data types by sampling and and
underscore sampling and it had Excel
support it had PDF support it had sip
support
and at the time it's introduction it was
really uh pushing the boundaries and it
was really good at the time you know
that was in 2013 10 years ago but it's
no longer actively maintained and
and that we found some issues with it
primarily that it was not tolerant of
data quality issues
so
when uh
back then we were even discussing you
know a lot of things in the community uh
somebody even proposed that maybe we
should work on message tables too
I also proposed that maybe hey we need
to make the data pusher multi-pass
so it can do a lot more robust uh
inferencing that's tolerant of data
quality issues and extract more metadata
and uh
you know I I had this laundry list of
things I wanted to do and this was back
in May 2015.
I said okay it will do four passes and
it will do all these things compute the
data dictionary extract metadata and
then uh do mapping and linking and all
that good stuff you know I was kind of
dreaming back then of what the potential
of data Pusher in the data store was
but unfortunately you know that didn't
happen and
the year after 2016 of my my our first
open data startup with Golia was some
was acquired by opengov
the good thing then is open gov being a
Silicon Valley venture-backed company we
had resources at our disposal and one of
the things we did was support the
development
of
what we thought was a Next Generation
beta Pusher which is X loader short for
Express loader
and it IT addresses the key uh
deficiencies of of data Pusher that we
thought was becoming the number one
source of support issues we were rolling
out secant implementations across the
country and oftentimes because we often
include
the data store
you know the data Pusher was not
really very robust and it was costing a
lot of support issues and it was really
slow so this was addressed by David Reed
who developed Express loader
we supported that development in opengub
and it was exponentially faster 10 times
faster
it was through bus
but at the cost of kind of dumbing down
the inferencing it didn't even attempt
to inference the data basically
basically it loaded everything as stacks
and it was faster because it was just
pumping the data using a posca SQL copy
you know it's a simpler queuing Tech and
it was simpler to deploy
so uh that was good in 2018
uh the year before
you know we we have some engagement in
the in the open data community and the
Civic analytics Network they came up
with a a wish list of things that they
wanted to to see in open Data Solutions
and this the Civic analytics network is
basically a Consortium of Chief data
officers and analytics principals in the
public sector uh I think they work out
of the Harvard Kennedy School of of
government Ash
Center for for government and they came
up with all these things that they did
they wanted to see in the open data
space you know
one thing I want to highlight here
move away from the single data set
Centric view geospatial data as a first
class data type
decrease the cost and work of publishing
data
large data sets
and all that you know and transparent
pricing and I think you know we answered
that because we're open source right
people can choose to deploy on their own
and in our case we have done some other
data pushing projects and experiments
preceding uh data Pusher plus so we we
worked with the city of San Antonio
Texas we built this thing called the
data pump basically it's a way to pump
iot time series data uh near real-time
data into CN
that is a good experiment where you
spanned us we use uh Json schema to do
validation of the data and it had a
simple queuing system as well and you
know it
for that specific use case of bumping
Time series
iot data it was good and you know it's
still being used
and also in in around this time frame we
we were engaged by uh
we were building a an Enterprise catalog
for hedge fund and as you can imagine a
hedge fund have all kinds of data
sources and they wanted a central
metadata catalog of where all the data
instrument you know it may it be in a
Mainframe may it be in Snowflake may it
be in a SQL Server database maybe
sitting in Dropbox they just wanted to
crawl order data assets and catalog it
and secant but one of the requirements
of that project was
can you extract metadata from these data
sources proactively without the data
publisher annotating the catalog or at
least give the data publisher the data
curator a head start by doing all these
inferencing
and we attempted to do that
we at first we use uh I'm sure if you
deal with data popular
dual CSV kit
but the thing with CSV kit was even
though it
it it had the capabilities it it was
memory constrained and it was slow
so we started using this other tool
called xsv which was written in Rust and
it was really fast but the problem was
uh
it wasn't actively maintained so we came
up with another rename Fork of ours we
called it the qsv and you can see my my
good art skills here
yeah
we're coming up with a logo
and before I jump into the demo
just a little check on where the secant
data store is I think like I was
dreaming in back in 2015 when we were
having that discussion on the GitHub
issues of secan
that the postgresql foundation of the
data store is largely in my mind in my
opinion underutilized you know we we're
not using bullshits or spacious spatial
queries
we're not doing you know like joins
we're not doing high performance SQL
queries even though you know the data
store search is there the store search
SQL is there
the data store currently
behaves more I mean again this is just
my opinion like a tabular blob store
you know it's it's a table
but it's kind of just like a blob there
that you can query on its own and do
free free text search uh full text
search
like
so
that's the ambition of data Pusher buses
to unlock this capabilities in tablet I
think there's relatively low hanging
capabilities and and we've done a lot of
projects where we do cataloging of
Enterprise data assets
and basically we were concentrating on
just the metadata but what if we take
secant to the next level and give it
more data Lake like capability so you
can instead of just cataloging metadata
you can start doing queries against
things that you elect to persist in the
data store
right what what capabilities are these
you know like smart data dictionaries
uh Native data types so not everything
is just text
or num or numeric instead of using the
the optimize postgresql data type data
enrichment like geocoding
uh data screening for piis data
validation you know perhaps
when something is updated it validates
the data in a performant manner
aliasing the the resources so you the
developer can refer to the
to the data sets using the secant data
store API in a you know an easier way
instead of just looking at the resource
ID
so this all goes back to that multi-pass
date Pusher which this I came up with
you know that four pass multi-pusher
back infinite for 15. I always had that
in the back of my mind
so we eventually got the opportunity to
work on data Pusher Plus
it was first released last year
basically it's a it's a fork of data
Pusher we combine The Best of Both
Worlds it has the inferencing of data
Pusher and it has a speed of of X loader
uh but the one thing we did uh choose to
replace replace message tables with USB
which is another thing we developed for
that for that for that hedge fund client
of ours
and also talking about clients we were
fortunate to have a client who was very
forward-looking uh the Texas natural
resources
agency uh we're building uh
we're building with them uh not for them
the Texas water data hub
and uh one of their use cases they did a
very exhaustive human-centered research
the year before they even started
writing code
that wanted to do
basically one of their key findings is
one of the best ways to improve the user
experience for the data publisher
it's to modernize the the the data
ingestion workflow
so you upload the resource first you
upload the data and then infer a lot of
the metadata from the from the data you
uploaded
and then
do the curation of the metadata if
you're familiar with secant you know
right now it's you upload you put in all
the metadata first and then you put in
the data right like
and there's there's no influencing
involved
so the key highlights of data Pusher
it's it has Bulletproof
uh Ultra fast data type inferencing
uh just to give you an idea uh say for
about uh 125 megabyte
uh CSV file
uh I I use the New York City 311 file as
an experiment that has 40 columns it has
you know like for that 125 megabyte I
think is uh
um
I think up two million rows it in first
it scans an adverse the data types
Bulletproof it's not guessing it's not
sampling in in less than a second
right because it does multi-threading it
does all these techniques to really you
know with with the with the fast
processors we have now CPU acceleration
and all that to
to do the inferencing
it also takes advantage of postcode SQL
copy so it's also fast in pumping the
data but it's not just pumping
all the data Stacks it's pumping it as a
proper data types because of the
Bulletproof inferencing
it's robust it's designed to be robust
so it's it's very tolerant of data
quality issues
right and if there's data quality issues
it in it informs the user right then and
there so they can
change it
tweak the data
and all this is made possible because we
were using that qsv U bus engine
so let's stop the slideware and go to a
demo
uh okay so let me just
oops
go here for now
Okay so
so here's Data before I launch into it
let me show some Salient points of data
pusher
uh let me make the hand
the the font bigger for people
it feels like that big
so first off uh we wanted it to be easy
to maintain it has a lot of knobs that
you can uh tweak
actually thank you to Tama I think
Thomas here we made the configuration
file easier
to configure but just by using a DOT EnV
so you can you can pass all these
parameters to to data Pusher plus some
parameters I want to call out
it has this Auto aliasing
so instead of referring to a resource as
a resource IP it it basically uses the
organization name the data set name and
the resource name as the Alias
automatically
summary statistics automatically if you
want to
it supports Excel
uh data Pusher no longer supports Excel
one that we have now it supports the
duping
right it supports Auto indexing
uh and uh you know it it can index based
on what it saw of the data so if there's
a threshold of okay they're more there
are only three values or more
do an index on on this data
it allows download previews so say you
have a 10 gigabyte file you're
harvesting
uh that will take forever right or it
will time out so if you do previews and
you're harvesting only a thousand rows
you only want a thousand rows and you
want to just point to the source it
supports that
uh in preview Rows 2 it has the
capability to say if you give it a
negative value
instead of just fetching the first 1000
rows it will fetch the last 1000 rounds
if say negative 1000 or negative 500
that's 500.
uh for our friends who prefer a
different date format it
it can tell you know instead of here in
the US you do y-y-mnd you can do bmy
uh and all these other things that from
the past right and it's easier to
configure now
so let me uh start data pusher
uh right now we're in development mode
and let me show my sandbox ccan instance
okay I'm here ready
so here's uh
a simple sandbox instance of data Pusher
of of secant so let me just create the
data set
so let's see now
uh
let me find some good data that we can
test against
okay
so let's see
okay so here's like a 48 megabyte file
let's see if we can ingest it
well the thing here is 48 megabytes
uploading so data push is not involved
yet at the moment
so if we look at the file it's still
doing its work
if we look at the data store tab
oh it's too large right okay here it
might be a uh I at the default right now
is 25 megabytes
and it's too large but I told you
earlier that we can do previews right so
let me turn that on
so let me modify the
that configuration
so let's download the preview only
let me start the data pusher
and let's try it again
ah we have a different result
okay as you notice now it's very chatty
uh the thing is it gives it it
tends to over give you a lot of
information so here it only said okay
and only downloading the first 1000 rows
I did that in so long then so much time
I'm normalizing the data and transcoding
it to utf-8 and validating the file it's
it's all form I'm checking the header
names if these are valid PostScript SQL
identifiers
it's checking for duplicates so there
were no duplicates found
uh it's inferring data types and
combining statistics
okay inferred all the data types right
and this all happened in a tenth of a
second
uh and then it copied a thousand rows to
the data store
and uh
two tenths of a second created an alias
the Alias is this
instead of this so when you call the
the secant API when you refer to the
resource ID instead of having this hash
you can
use this instead
and since summary statistics is enabled
it created another resource
characterizing is like like additional
metadata what the what it found
and it index things that it found to be
good candidates for indexing based on
the cardinality of that column right so
all these happen in 1.2 seconds so it's
fairly fast this is just running on uh
okay on uh on a VM that I that I have on
on my machine here
so and it did all that
and also like I said it created the
summary statistics automatically
so it inferred all these things right it
inferred okay this column is a string
this column is an integer this is It's a
float the minimum maximum the range
uh the variants and all these statistics
that we will in you know can leverage
later to do a lot more uh metadata
inferencing that we want to do
so you can see the data dictionary is
pre-populated right and the data data
types are correct
and it added some additional information
to the metadata so
so if you want to see
say you're using secant r or you're
using some
some program that's using that data and
you want to get the statistics you can
query secant and give me an asset to
give you this the summary statistics of
this resource the one we just uploaded
so you can do that so this almost
feels like tabular metadata right
so let me upload a more interesting data
set
3-1-1 data set is more interesting
okay so here we have 10 000 rows in tsv
format
[Music]
I guess we only had preview rows it only
did the first thousand rows right
but notice some things here
it infers
appropriately
uh
and if since I asked for
summary statistics it computed the
summary statistics for it as well
um
and notice to hear that it computed
statistics for
dates and date format
right and it normalizes all the dates it
finds you may have your dates in
different formats it normalizes them to
RFC 3339 or ISO format so it's database
ready
right so you can see the minimum maximum
ratio to create the date
uh and even all these other interesting
uh
no statistics also into recurrence as
state
so let's see now time check I can do a
more extensive demo but you know I look
forward to getting feedback from the
community on it uh just wanted to show
you what else is uh
on
on the plans of data Pusher plus so
right now with Texas uh
they support the development of data
Pusher plus the main reason we did that
was we wanted to enable this smart data
set upload workflow where you we start
data ingestion by basically starting
with the data uploading the data and
then that's why we did that that
inferencing and statistics compiling is
then what we want to do is pre-populate
all them as much metadata as possible
like Computing say the extent of
latitude longitude with a special extent
or computing
uh the valid values automatically
so we're doing that with them uh that's
currently what we're about to undertake
uh I didn't get a chance to demo this
but right now if the data dictionary is
even smarter so if you upload uh
uh say a CSV that's in val has invalid
column names that are not valid
identifiers and PostScript SQL it will
do something so it becomes valid and it
updates the data dictionary with the
original column name
another thing that we're looking to do
with another client
is uh to in now that the data store is
populated in an optimal way
because you want to improve the search
experience inside
when viewing the data right right now
when you view the data
uh you have the search
interface and what we want to do is do a
search Builder where you can do
interactive and or and based on the data
type you know have different conditions
between uh that makes sense for that
data type if you're doing it for a date
or for a number
um
data enrichment another thing that we
are aiming to do early this year is
give the ability to do geocoding so
right now
if I go to the data store the only
interface I have here is to basically
re-upload
right but what if we can fine-tune the
data Pusher job
for that for that data right so say it
has lat Longs or say it has addresses
then you want to get the lat loss we
want to put the more more of a
job customization interface here so I'll
say I want to request geocoding or I
want to request pii screening uh for
this data set so that that you can do it
here that's one of the things we want to
do
another thing that we want to do is do
data validation
so I was playing I was showing you
earlier that the 311 data set so one of
the things we did with USB 2 is be able
to infer now that we can do this
bulletproof inferencing and compiling of
statistics
we can infer the schema basically based
on the statistics and so we
automatically create the Json schema
and then you can validate other similar
CS uh data sets against that schema and
it's very fast it can in the then what
that three or one example it can
uh validate
300 000 records a second using a fairly
complex Json schema
another thing we want to do
is have a management in interface with
the data store so right now the the
you'll you have to go to the data store
tab of each resource that has an entry
in the data store we wanted something
that you know you can see all the the
things in the data store and look at the
attributes and maybe manage the uh
what's inside the data store change the
Alias we encode it do geocoding do up
search
um and also make it easier to deploy
so uh you know there was a contribution
from Mexico to containerize uh data
store plus so we want it very easy to
deploy uh and run as a web service
um
there's a long wish list once again so
if you go here a lot of things we want
to do a lot of things have been checked
off already
and we are hoping we're hoping that with
your help
uh your feedback your use cases your
requests your prop your problem your
projects your code all contributions are
welcome and we're sending out the survey
on what capabilities you'll want to see
first uh that no we can really build
something that takes together as a
community
the secant data store to the next level
so with that I end the demo and maybe we
can do the Q a
one yeah thank you do for the great
presentation
yes let's open the floor for questions
okay thank you very much
[Music]
um
my question is for second I have used it
before for uh
Service public service Tunisia it was
good we have used it with Linux Linux
system
for the validation of second windows and
can you explain it's uh there is a
version compatible with the other
systems
like like Linux
how we can use it
freely okay I can say
yeah I mean there are multiple ways to
get started on ckn and then if I'm
pronouncing your names
yes
thank you very much
so yeah there are multiple ways to do it
you know there's a package installed if
you just go to the instructions uh you
can do it yourself quickly if you're
familiar with Docker I think the the the
core team has created a Docker rice
implementation so you can just do a
Docker install without having to really
know what's under the hood
and also with uh
I think there's something in the AWS
Marketplace or link digital so multiple
ways to get started
uh asks for data Pusher plus we our aim
to is to make it easy to deploy so
uh it can work with even old
secant installations in the 2.6 2.7
series and above
um so yeah there are multiple ways to
just reach out to the community valid
again because I'm sure people will be
more than magical
thank you very much
okay I don't know how to pronounce your
nickname the zatilo oh yeah
thank you I am
two months old in the second Community
pretty fresh and I remember I am sick
and administrator in my organization and
it's also new here um actually the the
person who is supposed to to implemented
in this uh in my organization and I am
my my prob my question is about uh the
easiest and fastest and fastest way to
upload data from a postgres database to
seek and
we have a we have a large
we have a large data warehouse and I am
I am building now an eight year process
uh that I'm doing the transformation and
then the next step will be to derive
emails to upload this data directly into
the second environment
is this data Pusher Plus
uh
good uh a good alternative
for a beginner like me
uh it should be I mean this your use
case reminds me actually of uh what what
I we we did we tried doing with that
hedge fund right so basically they
wanted a central catalog of older data
exactly understood the source and they
didn't necessarily want to have
everything in secant because you know
it's a huge data sets but they wanted a
single source of fruits for the beta
data yeah but one thing they also wanted
was oh I just don't want the name and
the location and the version I want can
you describe to me the the structure can
you so that's why we the data Pusher
plus enables that uh so you you can you
can you can do this those other things
okay with the data Pusher Plus
is it already I mean in second or it's
an extension how do I how it's an
extension it's an extension and it's
meant to be uh it's meant to be a
drop-in replacement for a data Pusher
that you can you know just experiment
with without committing to it you can
still use
the traditional data push here you can
still use x loader if you want to but
yeah
it's another alternative to pushing data
into the secant data store
okay thank you very much
okay there are some questions in the
chat
um I don't know if the people would like
to
um ask
for example Carlo is asking about uh
okay Francisco you can go ahead and then
we go back to Carla
yeah thanks uh congrats on the
presentation Joe very nice work so in
one of the discussions you mentioned
that uh you wanted to have something
that could be maintained by its second
core uh so my question would be if you
consider using friculous pie framework
which appears to be kind of the success
sort of messy tables and kind of doing
the work of tsv so just to see if you
went up that road and what problems did
you uh yeah I actually explored the
fictionless framework uh but the main
criteria was performance right so
I wanted something because if you upload
the data and while you're filling out
the form like a few seconds later you'll
want the inferences to be there in the
form already
it has to be really performant
and
my my limited experiments with the
fictional framework and I didn't have
that it didn't have that performance
capability I mean the and also I I
didn't you know I'm no Russ well I can't
I'm kind of good across now when I
started uh qsv I wasn't good at
Roosevelt I was learning Russ but it if
you use Visual Studio code
right and if you try using the find
files feature of Visual Studio have you
I don't know if everybody's familiar
with Visual Studio to find files
uh the person who wrote the the search
engine behind that that returns like
searches like a whole directory in less
than a second
it's the same person who wrote
xsv which was the basis of qsv and I
just basically stood on the shoulders
and built on top of it because I was
scratching an itch because when I was
doing that project with that hedge fund
they were we were basically crawling all
their data in the Enterprise and they
wanted the they wanted the metadata
catalog updated every night
you know what was the latest model what
was the latest feed
and I was doing CSV kit it's written in
Python and it was not for me
SSV and it was fast and then but the
thing with accessory it wasn't regularly
maintained so with the permission of the
maintainer activated a fork and uh
that's
you know started from there but I'm open
uh actually uh the guy who wrote message
tables I forget his name kindly made a
contribution to qsv
uh talking about so it supports data
packages now it it publishes data
packages per K uh and connects directly
to postgresql and to into sqlite
uh so so yeah
thanks
thank you okay Thomas you can go ahead
oh thank you so I was really happy to
see the uh data Pusher plus comparison
we're using the vanilla data Pusher a
lot and we hit a lot of the limitation
you mentioned but I wanted to ask about
the plans or maybe current efforts to
try to make the data Pusher Plus
at the new default data Pusher for c-com
uh this is mainly considering about
long-term maintainability of the project
and I was wondering
um like is this a plan
um and if not what's preventing you from
doing this out of curiosity
no um we're big fans all the work you
know ever since we've been involved with
the secant Community we always uh
default to open
so when we when we migrated Boston from
soprada to secant
they want the data dictionaries
and
you know we instead of us building it
specifically for just Boston we work
with a corpsman a core member of the
team and to to surface that capability
for data dictionaries and it's part of
the secant core project
um
Boston also wanted a more modern tabular
interface and again Ian did some work
with data tables for Canada and we
wanted to they adjusted more so yeah so
we contributed back so and then the
for the work we're doing with Texas
right now it requires some work with
scheming
so the the data ingestion workflow is
more modern right so you right now if
you use scheming I don't know if you're
familiar with scheming it's a way to
customize the metadata schema of secan
to include more metadata Fields you
basically have one long form
right but the human-centered design
research done by Texas said okay we need
to break that up
right so we we sponsored some work again
so what basically I'm trying to say
Thomas is
I that's it has an open source license
it's Ford from data pusher
the goal of it was to be a drop-in
replacement
uh we supported the development of X
loader previously in the previous life
in open glove so yeah if I it's not my
decision it's with a core team to decide
on that but we've been doing secant for
more than 10 years now and uh yeah as
long as we're here we'll we'll maintain
it and but I actually one reason why I
wanted to do this presentation is to get
the community more involved
and you know if it becomes de facto
replacement then I'm happy if it becomes
official replacement I'm even happier
still but that's really not my decision
thanks
yeah you can go ahead if you want
otherwise we're we're going to see the
chat
somebody said something or no
it was me absolutely yes
I think all aggressive question
okay
is there a Best practice
for monitoring notification services for
jobs also without hear if there's a
migration path
to data Pusher from secant to you know
Harvesters okay
external monitoring notification
services so one thing we did with data
Pusher plus to
was if you look at data Pusher the
default for the job history was SQL
light
right which is really in transient yeah
so we made the default for data Pusher
plus uh another database in postgre SQL
and you saw all the chatty
messages it was
emitting it's also being captured in the
data Pusher jobs there's a data Pusher
jobs database it's all in there maybe we
can we can make the if you want to do
proactive monitoring we can make that uh
that not that more log friendly it also
creates a log file
uh so you can you can you know you
integrate that into your monitoring
a service if you're seeing some some
kind of loud monitoring service
notification service
um
yeah so hopefully that answers your
question you like
I can see there is two questions from
Carlo
um Carol would you like to
read them or should I read it for you
I'm not sure if
you're here okay
okay so the first question is um I see
latn
l-o-n is there other geospatial type
supported like for example
um is multi-polygonal supported that's
the first question and then
um do we have any support for some other
tabular storage type for example
bigquery or bigtable and is this
frictionless data table
okay during the presentation I think
yeah we're right so sorry I missed that
so uh one contribution by by
let me actually share my look at the the
project right now
um
so yeah uh one of the things we want to
support is you know other table formats
uh like parquet perhaps so you can do my
performance queries we're also looking
at the experiments done by the Natural
History Museum in the UK where they
change the data store backend to use
elasticsearch and they use mongodb
so that was an interesting experiment
that we actually want to learn from
hopefully uh and you know
uh when we start experimenting with that
uh and if you look at the data store
it's really nicely architected the way
they the designers in the register did
it because you can really replace the
back end
with alternate uh
uh backends So Right Now the default
back-end is supposed to SQL but like I
mentioned uh the UK Natural History
Museum changed their back end to a
combination of elasticsearch of mongodb
uh one project we're involved in
hopefully maybe
we're kind of still discussing it maybe
you know make it pop the snowflake
instead of of data Pusher of a poster
sequel for for really fast uh
performance
yeah
okay and there is one question from
Robert
Joe can you show how the data looks like
from the database site
okay well let me share my screen again
so let me uh
fire up like
can you see my screen
yes
okay
so you know in the on the database side
let's look at the
one table that we just spun in
so this wcp to the resource side dsd97
so if we look at postgresql
box
d97
you know it's just a regular
it's just a regular table
you know and
yeah it's just a regular table
right
that's the the great thing about the
data store it's a very thin interface
about posca sequel apart from the ID and
the full text in index for so you can do
search on it you know the rest is
basically this regular
uh people
um thank you sir
yep
okay we have one more question and then
I'm aware of time so we need to wrap up
maybe we'll have time for one or two
more questions and that will be it so
from Juan Carlos we have we're using
data Pusher to upload hdf files to
sikken what would be the best way to use
Exploder data Pusher Plus instead
uh I'm not familiar with hdf files Rd
Stabler formats or uh I'm not familiar
with it but if it's tabular format CSV
tsv Excel you can use data Pusher plus
and like I mentioned data Pusher plus is
meant to be
an independent web service uh that is a
drop in replacement for data pusher the
next loader so you can experiment with
it freely you know without committing to
it without really you know all you need
to do is
you don't even need to change it you
know if you go to your CK 99i file
you know you have all the data Pusher
configuration settings in there pointed
to where data Pusher plus is listening
and hopefully once our will have a
container version of data pressure plus
that we can just
install the docker quickly and just
change the configuration values quickly
and then you should be able to
experiment and try it out and actually
we look forward to your feedback
your contributions your requests and
maybe you know build the build it as a
community
great
um so yeah let's let's let's have one
more question if anybody have a there's
a question
go ahead
with big files yes Riyadh so yep
I did a lot of this thing with big files
and that's why we came up with that
preview function in the preview thing
uh we one of the things I actually want
to work on with big files is
say you have a 10 gigabyte file you want
to push in the data store but you only
want the last
10 000 rows right now it will have to
download all 10 gigabytes
and then get the ten thousand dollars
one thing we we're looking to do
this web service has to have this
capability a lot of web servers have the
capability to do what we call our
adventure quests so I can tell give me
the last 10 000 rows so it doesn't even
know the weapon the you know 10 gigabyte
files which is perfect for harvesting
right because often like in what we're
doing with Texas they don't want to
store all the data for all the entities
in the state they just want to harvest
all the data and give a sample
so so yeah so we've been doing a lot of
testing with the big files
and I just wanted to call out Eric Eric
actually found talking about
contributions yesterday he did a little
code review and he found a little uh
SQL injection vector
and I was able to correct it thanks to
his heads up thank you Eric
great so I think we need to wrap up
because we're running out of out of time
somebody is joining now but we are
basically closing wrong time zone
yeah
um we have the recording so everything
is fine
let me share my screen again
and actually you wanna I have a little
announcement before
great yes yeah so so actually uh we're
also doing a study with the University
of Pittsburgh for the how to make SEC
and
uh ecosystem more robust so there will
be workshops in in the near future in
the next few months
but where we want to engage members of
the community from different cohorts
from people who are operating they
report us for government data analysts
you know data journalists uh you know
people who are using data as regular
citizens or as people in the private
sector so you know we'll be coordinating
with with Joanna and you know we'll just
keep an eye out because we're working
actively on really creating all this
material for first so we want to engage
the community
yeah maybe we can even provide some
information on the second block and also
in the meeting document
okay so yeah that'll be great
okay
so yeah just I want to make an
announcement which is quite important
um
second product is hiring
um a python developer so if you're
interested in working
for second project you can
um send your CV and details to Stuart at
second.org with a cc to jobs day
Ethiopian
um and basically if you want to learn
more about the position and the
requirements there is a article on the
blog which is called passionate about
open source we're hiring it is included
in the presentation that you see on
screen and I just dropped the link to
the to this presentation so if you're
interested please
um if you want to present the second
monthly life
there is a link in the meeting note
um with the Forum you can fill out the
form I'll drop a link to the to this
form in the chat as well or you can
write directly to me
um at IWANNA
at datopian.com
um if you want to there are many ways
basically to get involved in the
community in the second Community one of
the ways is
you just understood my journal but also
if you for example need to get if you
want to get involved you can chat with
us on guitar or you can publish some
discussions on GitHub
um all the links are provided in the
presentation in the meeting note and
also you can go to second or community
where you find everything
everything that you need
um one thing I'm not really father is on
the call I think he used to be I'm not
sure if he still is but
um so second Tech team is finalizing the
upcoming second to 10 release which is a
culmination of almost two years of work
um and basically you can help move the
release forward
um generally by testing and other ways
which are described in the article on
the second block which is called getting
ready for second and third to 10.
um a link available in the presentation
of course
we can stay connected
um
we have a new LinkedIn page which is
maybe a couple of months already and we
appreciate new followers there so you
can follow us we share interesting
content
you can also check our blog on second
org
um our Twitter
and with that being said I think we're
ready to go thank you Joe for the great
presentation and thanks to everybody for
attending see you next month
thank you very much thank you everyone
thank you everyone
thank you
thank you very much it was a pleasure
Joanna thank you and see you next time
thank you
[Music]
foreign

